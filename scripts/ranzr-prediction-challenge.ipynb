{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras.applications.densenet import DenseNet121 \n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.models import load_model\n",
    "import os, sys\n",
    "import cv2\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT_DIR = \n",
    "DATA_SETS = '../input/ranzcr-clip-catheter-line-classification'\n",
    "#SCRIPTS = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = os.listdir(DATA_SETS)\n",
    "datasets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = os.path.join(DATA_SETS, 'train')\n",
    "submission = os.path.join(DATA_SETS,'sample_submission.csv')\n",
    "test = os.path.join(DATA_SETS, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_SETS+'/train.csv')\n",
    "print(f'Train Data CSV: {train_df.shape[0]}')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(submission)\n",
    "print(f'Sub CSV: {sub_df.shape[0]}')\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label column names\n",
    "label_cols = list(sub_df.columns[1:])\n",
    "image_labels = train_df[label_cols].values # will be used for train validation splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count up the number of instances of each class (drop non-class columns from the counts) \n",
    "class_counts = train_df.sum().drop(['StudyInstanceUID','PatientID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot the distribution of patients to check if there is any class imbalance in the dataset\n",
    "def plot_class_distributions(values, index):\n",
    "    sns.barplot(x=values, y=index)\n",
    "    plt.title('Distribution of classes for the patients')\n",
    "    plt.xlabel('Patient Count', fontsize=15)\n",
    "    plt.ylabel('Catheter Position', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distributions(class_counts.values, class_counts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = DATA_SETS + \"/train/\" + train_df['StudyInstanceUID'] + '.jpg'   \n",
    "test_images = DATA_SETS + \"/test/\" + sub_df['StudyInstanceUID'] + '.jpg'\n",
    "len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_image = os.path.join(DATA_SETS, \"train_annotations.csv\")\n",
    "data_image = pd.read_csv(data_image)\n",
    "data_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images randomly on a grid\n",
    "def display_images(image_ids, labels):\n",
    "    fig = plt.figure(figsize = (16,12))\n",
    "    \n",
    "    for index, (image_id, label) in enumerate(zip(image_ids,labels)):\n",
    "        plt.subplot(3,3, index+1)\n",
    "        image = image_id + '.jpg'\n",
    "        image = mpimg.imread(os.path.join(DATA_SETS, \"train\", image))\n",
    "        plt.title(label, fontsize=12)\n",
    "        plt.imshow(image,cmap='Greys')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = data_image.sample(9)\n",
    "image_ids = tmp_train[\"StudyInstanceUID\"].values\n",
    "labels = tmp_train['label'].values\n",
    "display_images(image_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we check for any kinds of data leakage between any two data sets, train-test, train-valid, valid-test\n",
    "def check_for_leakage(df1, df2, patient_col):\n",
    "    \n",
    "    df1_unique = set(df1[patient_col])\n",
    "    df2_unique = set(df2[patient_col])\n",
    "    \n",
    "    common_patients = df1_unique.intersection(df2_unique)\n",
    "    \n",
    "    return True if len(common_patients) > 0 else False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_leakage(train_df, sub_df, 'StudyInstanceUID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for making validation set from given train set\n",
    "def make_val_set(data, val_size):\n",
    "    val_patientid = random.sample(list(train_df['PatientID'].unique()),int(val_size*len(train_df['PatientID'].unique())))\n",
    "    df_train = data[~data['PatientID'].isin(val_patientid)]\n",
    "    df_val = data[data['PatientID'].isin(val_patientid)]\n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = make_val_set(train_df, val_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_leakage(df_train, df_val, 'PatientID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train Data Size: {df_train.shape[0]}')\n",
    "print(f'Validation Data Size: {df_val.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_DIR = DATA_SETS + '/train/'\n",
    "TEST_IMAGE_DIR = DATA_SETS + '/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without augmentation\n",
    "def get_train_generator(df, image_dir, image_id, label_names, shuffle=True, batch_size=32, seed=1, target_width=256, target_height=256):\n",
    "    print('Train Generator Preparation: ')\n",
    "    \n",
    "    # first we normalize the images\n",
    "    image_generator = ImageDataGenerator(\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True\n",
    "    )\n",
    "    \n",
    "    # next we flow from data frame with a certain batch size.\n",
    "    # This flows the images from the directory\n",
    "    generator = image_generator.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        directory=image_dir,\n",
    "        x_col=image_id,\n",
    "        y_col=label_names,\n",
    "        class_mode=\"raw\",\n",
    "        classes=label_names,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=1,\n",
    "        target_size=(target_width, target_height)\n",
    "    )\n",
    "    \n",
    "    return generator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image generator\n",
    "def get_image_generator(train_df, image_dir, image_id, label_names, shuffle=True, batch_size=32, seed=1, target_width=256, target_height=256, sample_size=100):\n",
    "    # generator to sample dataset\n",
    "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=image_dir,\n",
    "        x_col=image_id,\n",
    "        y_col=label_names,\n",
    "        class_mode='raw',\n",
    "        batch_size=sample_size,\n",
    "        shuffle=shuffle,\n",
    "        target_size=(target_width, target_height)\n",
    "    )\n",
    "    \n",
    "    batch = raw_train_generator.next() \n",
    "    data_sample = batch[0] \n",
    "    \n",
    "    # use sample to fit mean and std for test set generator \n",
    "    image_generator = ImageDataGenerator( \n",
    "        featurewise_center=True, \n",
    "        featurewise_std_normalization= True) \n",
    "    \n",
    "    # fit generator to sample from training data \n",
    "    image_generator.fit(data_sample)\n",
    "    \n",
    "    return image_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(df, image_dir, x_col, y_cols, image_generator, batch_size=32, seed=1, target_width = 256, target_height = 256):\n",
    "    # get valid generator \n",
    "    generator = image_generator.flow_from_dataframe( \n",
    "        dataframe=df, \n",
    "        directory=image_dir, \n",
    "        x_col=x_col, y_col=y_cols, \n",
    "        class_mode=\"raw\", \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        seed=seed, \n",
    "        target_size=(target_width,target_height)\n",
    "    )\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ext(fn, ext='.jpg'):\n",
    "    return fn+ext\n",
    "\n",
    "train_df_splitted['StudyInstanceUID'] = train_df_splitted['StudyInstanceUID'].apply(append_ext)\n",
    "valid_df['StudyInstanceUID'] = valid_df['StudyInstanceUID'].apply(append_ext)\n",
    "sub_df['StudyInstanceUID'] = sub_df['StudyInstanceUID'].apply(append_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = get_train_generator(train_df_splitted, TRAIN_IMAGE_DIR, \"StudyInstanceUID\", label_cols)\n",
    "raw_image_generator = get_image_generator(train_df_splitted, TRAIN_IMAGE_DIR, \"StudyInstanceUID\", label_cols)\n",
    "valid_generator = get_generator(valid_df, TRAIN_IMAGE_DIR, \"StudyInstanceUID\", label_cols, raw_image_generator)\n",
    "test_generator= get_generator(sub_df, TEST_IMAGE_DIR, \"StudyInstanceUID\", label_cols, raw_image_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
